{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6bd4f0-3d46-4da1-a101-22dfd61a9fe1",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval\n",
    "<p>The primary focus of the notebook is to illustrate the process of using a transformer model to embed text data into a numerical representation that can be used to calculated a similarity score as compared to a query string embedding. Additionally, we explore some of the architectural theory of a complete application.</p>\n",
    "<p>Smart Document Retrieval can be divided into the following key components:\n",
    "    <li>1) Document Management</li>\n",
    "    <li>2) Source Text Extraction</li>\n",
    "    <li>3) Source Text Storage</li>\n",
    "    <li>4) Source Text Embedding\n",
    "        <ul>\n",
    "            <li>4a) Model Selection</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>5) Source Embedding Storage and Management</li>\n",
    "    <li>6) Query String Embedding</li>\n",
    "    <li>7) Similarity Scoring and Ranking</li>\n",
    "    <li>8) Advanced techniques <a href='https://www.sbert.net/examples/applications/retrieve_rerank/README.html'>Retrieval and Re-Ranking - Bi-Encoders(Retrieval) and Cross-Encoders(Re-Ranker)</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9804333-81ef-4abd-b6d9-a2b82bcb48fd",
   "metadata": {},
   "source": [
    "## 1) Document Management\n",
    "<p>The exact manner you manage the documents/resources to use will be based on your use case and is beyond the scope of this notebook; however, it is important to consider several items.\n",
    "    <li><b>Access</b>\n",
    "        <ul>\n",
    "            <li>With the application have continuous access to source documents?</li>\n",
    "            <li>Will the application need privileged permissions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Versioning</b>\n",
    "        <ul>\n",
    "            <li>Is there a document versioning process?</li>\n",
    "            <li>Are there duplicate/variations of a documents?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Document/Resource Types</b>\n",
    "        <ul>\n",
    "            <li>What type of document formats will be used? (e.g., MS Word, Excel, Google Docs, Websites, Emails, etc.)</li>\n",
    "            <li>Are there different format versions?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3d93e-51c3-4530-8662-8474e2c1568e",
   "metadata": {},
   "source": [
    "## 2) Source Text Extraction\n",
    "<p>The process for extracting the source text will vary by use case, we offer some things to consider during your design but there may be other considerations based on your requirements. The example dataset used in this notebook was extracted from USPTO patent XML files selecting just the abstract for embedding.\n",
    "    <li><b>Content Extraction - Technical</b>\n",
    "        <ul>\n",
    "            <li>How will you access the source text within the resource/document?</li>\n",
    "            <li>What libraries / tools will be needed to extract text?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Selection</b>\n",
    "        <ul>\n",
    "            <li>What parts of the document will be selected for extraction? (e.g., Subject Line, Executive Summary, Individual Sections, etc.)</li>\n",
    "            <li>If you would like the application to identify specific locations within a document that contain the relevant information you will need to extract source text at the same level.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Content Quality</b>\n",
    "        <ul>\n",
    "            <li>Do you need to remove meta-data or file formatting components such as XML tags?</li>\n",
    "            <li>Are there errors that need to be fixed? (e.g., spelling, formatting)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1556ad-8ef2-4c04-8d22-229c396760d4",
   "metadata": {},
   "source": [
    "## 3) Source Text Storage\n",
    "<p>The example dataset used in this notebook has been stored in a simple parquet file format however if your use case needs to scale to millions, billions, or more items a database may be beneficial. One option could be to use MongoDB running in its own container to store the Source Text data.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e0ca33-9f08-4a85-b7ce-1974d3ee65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Importing the needed libraries & Modules\n",
    "\n",
    "# Import cudf. cudf is part of the NVIDIA RAPIDS datascience SDK and is used to store the dataframes \n",
    "# used in gpu memory.\n",
    "import cudf\n",
    "\n",
    "# Import SentenceTransformer and util from the HuggingFace sentence_transformer library which has\n",
    "# been pre-installed in this environment.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import pickle. pickle is used to store the embedding\n",
    "import pickle\n",
    "\n",
    "# Import Path. Used to manage file system\n",
    "from pathlib import Path\n",
    "\n",
    "# Import smart_search_models. This module was created for this example to simplify the management of the \n",
    "# various models that can be used for the embedding process.\n",
    "import smart_search\n",
    "\n",
    "# Set some notebook variables\n",
    "DATASET_NAME = \"enron\"\n",
    "EMBEDDING_FOLDER = \"embeddings/\"\n",
    "RUN_EXAMPLE_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695299cc-bb30-4a1d-9ee8-0c504706a88b",
   "metadata": {},
   "source": [
    "### Loading Example Dataset\n",
    "<p>The dataset being used in this example is comprised of over 500,000 email messages from the Enron dataset. The dataset has been downloaded and extracted using the included <a href='gather_enron_dataset.ipynb'>gather_enron_dataset</a> notebook and parsed using <a href='parse_enron_data.ipynb'>parse_enron_data</a> notebook</p>\n",
    "\n",
    "<p>The source text dataset is stored in a parquet file containing the following:\n",
    "    <li><b>file_path:</b> source file path from the Enron dataset\n",
    "    <li><b>message_id:</b> unique id assigned in the Enron dataset\n",
    "    <li><b>date:</b> email date\n",
    "    <li><b>from_address:</b> email address of the parent email\n",
    "    <li><b>to_address:</b> destination email address in the parent email\n",
    "    <li><b>org_filename:</b> name of the file source for the enron dataset\n",
    "    <li><b>is_reply_forward:</b> flag indicating if the message was a reply or forward\n",
    "    <li><b>message:</b> email message content\n",
    "    \n",
    "</p>\n",
    "<p>The method of storing the source text may vary based on your use case (e.g. CSV, parquet, JSON, MongoDB, etc..). We use a parquet file in this example for simplicity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8e6913-d9eb-4701-baf0-6ddd69ddd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 517415 entrees\n",
      "CPU times: user 659 ms, sys: 272 ms, total: 931 ms\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>from_address</th>\n",
       "      <th>to_address</th>\n",
       "      <th>org_filename</th>\n",
       "      <th>is_reply_forward</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/maildir/richey-c/1.</td>\n",
       "      <td>&lt;8628846.1075841459361.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 10 Jul 2001 11:01:51 -0700 (PDT)</td>\n",
       "      <td>karim.punja@enron.com</td>\n",
       "      <td>cooper.richey@enron.com</td>\n",
       "      <td>cooper richey 6-26-02.PST</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nCooper,\\n\\n\\nHere is the way I think of wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file_path                                    message_id  \\\n",
       "0  data/maildir/richey-c/1.  <8628846.1075841459361.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    date           from_address  \\\n",
       "0  Tue, 10 Jul 2001 11:01:51 -0700 (PDT)  karim.punja@enron.com   \n",
       "\n",
       "                to_address               org_filename  is_reply_forward  \\\n",
       "0  cooper.richey@enron.com  cooper richey 6-26-02.PST             False   \n",
       "\n",
       "                                             message  \n",
       "0  \\n\\nCooper,\\n\\n\\nHere is the way I think of wh...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Load in the example dataset\n",
    "df = cudf.read_parquet(\"data/enron_extracted/email_data.parquet\")\n",
    "print(\"The dataset contains {} entrees\".format(df.shape[0]))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522c0a4-64de-45a1-a2d4-61ad96835051",
   "metadata": {},
   "source": [
    "## 4) Source Text Embedding\n",
    "<p>Historical methods for search involved simple <a href='https://en.wikipedia.org/wiki/Lexicography'>lexicographical</a> similarity pattern matching such as <a href='https://en.wikipedia.org/wiki/Regular_expression'>regex</a>. Although methods such as lexical search can be useful for some use cases, they have several disadvantages such as needing to specific the precise terms to search for. To improve search results it can be advantageous to search based on <a href='https://en.wikipedia.org/wiki/Semantic_similarity#:~:text=Semantic%20similarity%20is%20a%20metric,as%20opposed%20to%20lexicographical%20similarity.'>sematic similarity</a> using concepts rather than word for comparison.</p>\n",
    "\n",
    "<p>To be able to search by concept we must be able to represent our data in the form of concepts. This is where <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> come in. <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> are a form of Machine Learning that can be applied to Natural Language Processing (NLP), the models have been trained on extremely large datasets such as Wikipedia to develop the ability to represent input text as a highly dimensional numerical representation, this process is called <a href='https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization'>embedding</a>. If this sounds complicated, don't worry the hard parts are all abstracted away for us, we just need to use the sentence transformer library. Although there are benefits of understanding how the models work, sometimes it can be just as valuable to show how easy they are to use and how impressive the results can be using off-the-shelf models. If greater accuracy is needed you can always <a href='https://www.sbert.net/docs/training/overview.html'>train transformers</a> on your own datasets to improve their capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e3398-8aa2-49c4-b71f-f4da717f990d",
   "metadata": {},
   "source": [
    "### 4a) Model Selection\n",
    "<p> There are a large number of models to choose from on <a href='https://huggingface.co/'>HuggingFace</a> even for just the task of <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a>(>800 as of 11/2022). We have included a python module to help simplify organization and selection of a smaller subset of models to experiment with (~100). Using <a href='https://huggingface.co/'>HuggingFace</a> simplifies the process of downloading and running the various models, it is not the only way to consume Transformers but it was chosen as it is one of the easiest ways to get started.</p>\n",
    "    \n",
    "There are several areas to consider when selecting a model for a given task\n",
    "<li><b>Model Size</b> - Large models need more VRAM and can take longer to run but may be more 'accurate'</li>\n",
    "<li><b>Model Architecture</b> - Some models might be designed for specific use cases or finetuned for a given problem. If your use case is similar, you might have high performance out of the box.</li>\n",
    "<li><b>Task</b> - Different models have been trained for different tasks. Some examples of various tasks include; Semantic Similarity, Semantic Search, Questioning and Answering, and Document Summarization. \n",
    "    \n",
    "<p>As stated above, the models have been trained to solve a specific workflow. In our case we are trying to identify Semantically Similar documents to our query string. Within the Semantic Similarity group there are subgroups of tasks. These tasks include identifying semantically similar sentences where we try to evaluate two or more sentences and score their similarity. When the elements being evaluated are of similar length (sentence to sentence, paragraph to parapraph) the process is called <b>symmetric semantic search</b>. If you are evaluating a short query phrase or word to sentance, paragraphs, or even documents it is refered to as <b>asymmetric semantic search</b> and models have been specially trained for each type.</p>\n",
    "    \n",
    "<li><a href='https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models'>Symmetric Semantic Search Pretrained Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/pretrained-models/msmarco-v3.html'>Asymmetric Semantic Search</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d366e-220d-4917-9395-a40fdc9dba92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the Model\n",
    "<p>Loading the model is a simple as passing the model's name as an input argument to create a model object. If the model isn't available locally it will be downloaded automatically. One of the hardest parts of working with HuggingFace is keeping track of all the models available. You can view all the models available for <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a> and copy the name into the code or to simplify things we have created a very basic python module <a href='smart_search.py'>smart_search.py</a> to hold model names.</p>\n",
    "\n",
    "<details>\n",
    "  <summary>SentenceTransformer Parameters</summary>\n",
    "<li><b>model_name_or_path</b> – If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.</li>\n",
    "<li><b>modules</b> – This parameter can be used to create custom SentenceTransformer models from scratch.</li>\n",
    "<li><b>device</b> – Device (like ‘cuda’ / ‘cpu’) that should be used for computation. If None, checks if a GPU can be used.</li>\n",
    "<li><b>cache_folder</b> – Path to store models</li>\n",
    "<li><b>use_auth_token</b> – HuggingFace authentication token to download private models.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d3181b-83c9-4868-80f9-e88194092c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: 'msmarco-distilbert-base-v4'\n"
     ]
    }
   ],
   "source": [
    "# Select and load model.\n",
    "# Note: If a given model hasn't been used since the container has been loaded it will be downloaded automatically.\n",
    "\n",
    "# The sentence_models list is a large list of models. They have not been grouped by task beyond sentence similarity \n",
    "#model_name = smart_search_models.sentence_models[6]\n",
    "#model_name = smart_search_models.default_model\n",
    "\n",
    "# asymmetric_cosine_similarity_models are special purpose models for Asymmetric Semantic Similarity through cosine similarity calculations\n",
    "model_name = smart_search.asymmetric_cosine_similarity_models[0]\n",
    "\n",
    "# symmetric_models are special purpose models for Symmetric Sematic Similarity\n",
    "#model_name = smart_search.symmetric_models[2]\n",
    "\n",
    "print(\"Loading model: '{}'\".format(model_name))\n",
    "model = SentenceTransformer(model_name,cache_folder='./models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4a8b-190e-46c2-9af3-d1b37a0f6aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source Text Embedding\n",
    "<p>To embed the source text, we can pass the entire column of our dataset into the model object in a single line of code as shown in the cell blocks below.</p>\n",
    "\n",
    "<p>A couple important items to note here:\n",
    "    <li>You only need to embed the source text once for a given model. Depending on your use case you may wish to database the embeddings for later use, just remember to keep track of the model used for embedding and the source document.</li>\n",
    "    <li>As each model will embed the input text differently you need to ensure the source text and query text were embedded using the same model. If you choose to database or store your embedding for later just be sure to track which models were used for the embedding as you will likely get unexpected results if comparing embedding from different models.</li>\n",
    "    </p>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>encode Parameters</summary>\n",
    "    <li><b>sentences</b> – the sentences to embed</li>\n",
    "    <li><b>batch_size</b> – the batch size used for the computation</li>\n",
    "    <li><b>show_progress_bar</b> – Output a progress bar when encode sentences</li>\n",
    "    <li><b>output_value</b> – Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values</li>\n",
    "    <li><b>convert_to_numpy</b> – If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.</li>\n",
    "    <li><b>convert_to_tensor</b> – If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy</li>\n",
    "    <li><b>device</b> – Which torch.device to use for the computation</li>\n",
    "    <li><b>normalize_embeddings</b> – If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba9e40-5d4a-4a73-a771-97c295b7875d",
   "metadata": {},
   "source": [
    "#### Performance Considerations\n",
    "The examples below show the benifits of processing an array of inputs versus iterating over a loop. Additional performance improvements can be found by utilizing Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b61bd18-37b5-41a5-8c1a-128f32330dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset to be used as an example\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    example_size = 10000\n",
    "    example_df = df[0:example_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e3b37-71b8-42a0-b398-ee75d7f3471f",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder for every message. This does not take advantage parrallel processing and we can see the processing time difference. The timings are a result of test runs using an NVIDIA RTX A6000.</p>\n",
    "\n",
    "<li>Model: all-mpnet-base-v2</li>\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Wall Time: 2min 13s --> 133 Seconds</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b83777-9526-4cfd-865b-dd9b98f8029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    # Initialize embedding list\n",
    "    source_embedding_loop = []\n",
    "    \n",
    "    # Loop though the examples and embed each message\n",
    "    for i in range(0,example_df.shape[0]):   \n",
    "        source_embedding_loop.append(model.encode(example_df['message'][i], convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3c877-8d23-46cf-bb45-5b996f15b25b",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder with the entire array. This method allows us to take advantage of batch processing. The timings are a result of test runs using an NVIDIA RTX A6000. Note we can run various batch sizes.</p>\n",
    "\n",
    "<p>Model: all-mpnet-base-v2\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Batch Size: 32 - Wall Time: 34.5 Seconds</li></p>\n",
    "\n",
    "<p>\n",
    "Model: msmarco-distilbert-base-v4\n",
    "<li>Example Size: 10,000 Messages</li>\n",
    "<li>Batch Size: 32 - Wall Time: 18.4 Seconds</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884034a5-7248-496a-80df-b24118ab1769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RUN_EXAMPLE_DATASET:\n",
    "    source_embeddings_array = model.encode(example_df.message.to_pandas(),convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cc650-f624-41df-94f3-aafa8e7d4dc6",
   "metadata": {},
   "source": [
    "<p>Note that the performance was 4x, this could have a significant impact if running on the full 500,000 message.</p>\n",
    "\n",
    "Model: msmarco-distilbert-base-v4\n",
    "Corpus Size: 500,000 Messages\n",
    "Batch Size: 64\n",
    "Wall Time: 18min 5s\n",
    "\n",
    "Note: Batch size does not seem to have a significant effect on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f289d0e-c1d9-47f5-8b0c-15b8a2314eae",
   "metadata": {},
   "source": [
    "### Embedding the entire dataset\n",
    "We only need to embed the entire dataset once. We can check if the model / dataset embeddings already exist. If so just load them from disk. If not, process them. This can take as long as 30 minutes to embed the ~500,000 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5c8345-4afa-4716-95fb-cefe3a2ae06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions to read and write embedding to files.\n",
    "def load_embeddings(embedding_file_path):\n",
    "        \n",
    "    #Load sentences & embeddings from disc\n",
    "    with open(embedding_file_path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_message_id = stored_data['message_id']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "\n",
    "    # As of now we only need the stored embeddings\n",
    "    return stored_embeddings\n",
    "\n",
    "def write_embeddings(embedding_folder, embedding_file_name,message_ids,source_embeddings):\n",
    "   \n",
    "    # Check if directory exits\n",
    "    dir_path = Path(embedding_folder)\n",
    "    \n",
    "    if not dir_path.is_dir():\n",
    "        print(\"Directory does not exist. Creating it now.\")\n",
    "        # If the directory doesn't exist create it.\n",
    "        dir_path.mkdir()\n",
    "        \n",
    "    # Create the file path\n",
    "    file_path = embedding_folder + embedding_file_name\n",
    "    \n",
    "    # Write out the embedding and message_id to disk\n",
    "    with open(file_path, \"wb\") as fOut:\n",
    "        pickle.dump({'message_id': message_ids, 'embeddings': source_embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b1b71a7-1f7a-4666-b7cb-fd78f261eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file exists. Loading it now.\n",
      "embeddings/embeddings_enron_msmarco-distilbert-base-v4.pkl\n",
      "CPU times: user 744 ms, sys: 1.32 s, total: 2.06 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Flag for multi-gpu embedding.\n",
    "TRAIN_MULTI = False\n",
    "\n",
    "# Create the file name that would be used to store the embeddings.\n",
    "embedding_file_name = \"embeddings_{}_{}.pkl\".format(DATASET_NAME,model_name)\n",
    "\n",
    "# Create embedding Path object\n",
    "embedding_file = Path(EMBEDDING_FOLDER + embedding_file_name)\n",
    "\n",
    "# Check if the file \n",
    "if embedding_file.is_file():\n",
    "    # If a file exists with the embedding file for this dataset / model combination exists load it.\n",
    "    print(\"Embedding file exists. Loading it now.\")\n",
    "    source_embeddings = load_embeddings(embedding_file)\n",
    "else:\n",
    "    # If an embedding file does not exist. Embed the dataset and cache the data.\n",
    "    print(\"Embedding file does not exist. Creating now.\")\n",
    "    \n",
    "    if TRAIN_MULTI:\n",
    "        pool = model.start_multi_process_pool()\n",
    "        source_embeddings = model.encode_multi_process(df.message.to_pandas(),pool)\n",
    "        model.stop_multi_process_pool(pool)\n",
    "    else:\n",
    "        source_embeddings = model.encode(df.message.to_pandas(),convert_to_tensor=True,show_progress_bar=True)\n",
    "    \n",
    "    # Write out the generated embeddings\n",
    "    write_embeddings(EMBEDDING_FOLDER,embedding_file_name,df.message_id.to_pandas(),source_embeddings)\n",
    "    \n",
    "print(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1683d3-da88-4396-a25e-4b8369b430bb",
   "metadata": {},
   "source": [
    "### Timinings\n",
    "<li>embeddings_enron_msmarco-roberta-base-v3.pkl - 34min 24s</li>\n",
    "<li>embeddings_enron_msmarco-distilbert-base-v3.pkl - 21min 55s</li>\n",
    "<li>embeddings_enron_multi-qa-mpnet-base-dot-v1 - 45min 29s</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b6bc1-f0a3-4c1f-946f-529bba635dcd",
   "metadata": {},
   "source": [
    "## 6) Query String Embedding\n",
    "<p>Using the same model, we then embed our query string to be used for comparison.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c44b46-0319-4f84-8f58-d417b5684428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 393 ms, sys: 179 ms, total: 571 ms\n",
      "Wall time: 803 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the query string\n",
    "query_string = 'having a baby'\n",
    "query_embedding = model.encode(query_string,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722045ab-8aad-4303-88ae-4663229769c5",
   "metadata": {},
   "source": [
    "## 7) Similarity Scoring and Ranking\n",
    "<p>Next, we need to calculate the similarity between the query embedding and all the source text embeddings. One of the most common approaches is to calculate the cosine similarity. Again, the complexities and math have been abstracted here with the <a href='https://www.sbert.net/docs/package_reference/util.html'>util.cos_sim</a> and sematic_search functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1fbabb-c332-44e6-a660-895343288bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.59 ms, sys: 4.55 ms, total: 13.1 ms\n",
      "Wall time: 18.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set k as the number of top results\n",
    "k = 100\n",
    "\n",
    "# Using the util function to run semantic search, default to cosine\n",
    "topk_results = util.semantic_search(query_embedding, source_embeddings, top_k=k)[0]\n",
    "\n",
    "# Extract the result ids\n",
    "topk_results_ids = [result['corpus_id'] for result in topk_results]\n",
    "\n",
    "# Get a dataframe of the top k results\n",
    "topk_df = df.iloc[topk_results_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eba2292-b9b3-4887-a726-36e75f8d4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to take care of my baby.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the message. \n",
    "import re\n",
    "\n",
    "# Using re to clean up empty lines\n",
    "msg = re.sub('\\n\\n', '', topk_df.message[0])\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a7017-3256-4da1-b08a-c011fffebda1",
   "metadata": {},
   "source": [
    "## Advanced Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb860166-738c-455e-bb44-5f029910bdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d72721c2bc4954b5627d40ca02e449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258b88cdbea04ca98bce4e5886d55470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be896edc9ddd4697aa6959a1da09f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5018789ea041a3af0b525791dc3390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08c8e4df9b6430bbac85d705a4d4969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the cross encoder library\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Load the cross encoder model\n",
    "cross_model = CrossEncoder(smart_search.cross_encoder_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5b121a-a62b-4b87-9aae-8099ebda388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 710 ms, sys: 0 ns, total: 710 ms\n",
      "Wall time: 716 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate the cross-encoder scores and assign scores to dataframe column\n",
    "topk_df['score'] = [cross_model.predict([query_string,msg]) for msg in topk_df.message.to_pandas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5275255a-8329-4130-9c82-98a1f7023b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe based on descending score\n",
    "topk_df = topk_df.sort_values('score',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e4268fb-4ce2-4e4a-af20-c570f5155d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Many of you know that my wife, Sue, and I are expecting a baby any day now.\n",
      "When Sue stopped by the office on Tuesday, we were both pleasantly surprised\n",
      "by the wonderful gift basket and gift certificate to -- you guessed it --\n",
      "Baby's R Us that the people on the floor gave to us.  The sage advise and\n",
      "warm wishes expressed on the card were particularly appreciated.  Thank you\n",
      "so much.\n",
      "\n",
      "Tim Belden\n",
      "\n",
      "ps - it's a boy\n"
     ]
    }
   ],
   "source": [
    "# Print the top result\n",
    "print(topk_df.message[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e1a0f-ac14-45cc-babd-856acd2416ab",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "<li><a href='https://huggingface.co/'>HuggingFace</a></li>\n",
    "<li><a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/usage/semantic_textual_similarity.html'>Sematic Textual Similarity</a></li>\n",
    "\n",
    "### Environment\n",
    "This notebook has been developed and tested on the following:\n",
    "<li>RAPIDS - rapidsai-core:22.10-cuda11.5-base-ubuntu20.04-py3.9</li>\n",
    "<li>Pytorch 1.12.1</li>\n",
    "<li>sentence-transformers</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
